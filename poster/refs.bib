@article{hopfield:1982,
  author = {J. J. Hopfield},
  title = {Neural networks and physical systems with emergent collective
computational abilities},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  year = 1982,
  month = {April},
  volume = {79},
  pages = {2554-2558}
}

@article{fncom:2017,
  author = {Folli, Viola and Leonetti, Marco and Ruocco, Giancarlo},
  title = {On the Maximum Storage Capacity of the Hopfield Model},
  journal = {Frontiers in Computational Neuroscience},
  volume = {10},
  year = {2017},
  doi = {10.3389/fncom.2016.00144},
  issn = {1662-5188}
}

@misc{ramsauer:2021,
      title={Hopfield Networks is All You Need}, 
      author={Hubert Ramsauer and Bernhard Schäfl and Johannes Lehner and Philipp Seidl and Michael Widrich and Thomas Adler and Lukas Gruber and Markus Holzleitner and Milena Pavlović and Geir Kjetil Sandve and Victor Greiff and David Kreil and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
      year={2021},
      eprint={2008.02217},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@InProceedings{storkey:1997,
  author="Storkey, Amos",
  editor="Gerstner, Wulfram
and Germond, Alain
and Hasler, Martin
and Nicoud, Jean-Daniel",
  title="Increasing the capacity of a hopfield network without sacrificing functionality",
  booktitle="Artificial Neural Networks --- ICANN'97",
  year="1997",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="451--456",
  abstract="Hopfield networks are commonly trained by one of two algorithms. The simplest of these is the Hebb rule, which has a low absolute capacity of n/(2ln n), where n is the total number of neurons. This capacity can be increased to n by using the pseudo-inverse rule. However, capacity is not the only consideration. It is important for rules to be local (the weight of a synapse depends ony on information available to the two neurons it connects), incremental (learning a new pattern can be done knowing only the old weight matrix and not the actual patterns stored) and immediate (the learning process is not a limit process). The Hebbian rule is all of these, but the pseudo-inverse is never incremental, and local only if not immediate. The question addressed by this paper is, `Can the capacity of the Hebbian rule be increased without losing locality, incrementality or immediacy?'",
  isbn="978-3-540-69620-9"
}