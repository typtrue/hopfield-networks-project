\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=custom,width=120,height=72,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{ox}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.14}
\usepackage{anyfontsize}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{Using Hopfield Networks to restore corrupted greyscale images}

% ====================
% Footer
% ====================

\footercontent{ \hfill \hfill
  Daniel Haywood}

% ====================
% Logo
% ====================
\logoleft{\includegraphics[height=4cm]{logos/Imperial-college-white.png}}

% ====================
% Body
% ====================

\begin{document}



\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{An introduction to Hopfield Networks}

    \textbf{Hopfield Networks} are a form of Ising model that have many uses in various sectors: 
    computational neuroscience, optimisation, and more. Similar to an Ising model, they
    are comprised of an array of nodes that are given a \textit{magnetic spin},
    either $+1$ or $-1$, and connections, \textit{'weights'}, linking them.
    We can use this archetecture to \textit{'train'} the network on given sets of data,
    effectively \textit{'storing the memory'} of the data on the network.
    Then, we can recover the stored data by inputting a new set of data
    and \textit{'updating'} the network using the weights we obtained from training
    it on our sets of data.

    % \begin{figure}
    %   \centering
    %   \begin{tikzpicture}[scale=6]
    %     \draw[step=0.25cm,color=gray] (-1,-1) grid (1,1);
    %     \draw (1,0) -- (0.2,0.2) -- (0,1) -- (-0.2,0.2) -- (-1,0)
    %       -- (-0.2,-0.2) -- (0,-1) -- (0.2,-0.2) -- cycle;
    %   \end{tikzpicture}
    %   \caption{A figure caption.}
    % \end{figure}

    In this project, we will look specifically at its application in image processing: how it can
    be used to restore and discern between corrupted pixellated images. By training the network on
    a set of images, we can then input corrupted versions of the images into the network, and updating
    the network will yield an image that should hopefully resemble the orignal image to a good degree
    of accuracy.

  \end{block}

  \begin{block}{Basic definitions}

    We start by defining our $n$ node spins $V_i$, where $i \in \{0, 1, ..., n\}$.
    These nodes all have connections to other nodes, and this is denoted by
    $T_{ij}$, which represents the connection between nodes $i$ and $j$. We note that,
    $\forall i, j, T_{ij} = T_{ji}$. Further, $T_{ij} = T_{ji} = 0$ whenever
    nodes $i$ and $j$ are not connected, or when $i = j$.

    Like an Ising model, we \textit{'update'} the network given a set of spins by
    considering a threshold for each node, denoted by $U_i$. Then, we say:

    \begin{equation} \label{update_rule}
      V_i = \begin{cases} 
              +1, & \sum_{j}T_{ij}V_j > U_i \\
              -1, & \sum_{j}T_{ij}V_j \leq U_i
            \end{cases}
    \end{equation}

    This threshold vector could be defined specifically to influence how specific nodes
    tend to behave, but for the purposes of this project we will define the threshold
    vector to be the zero vector, that is, $U_i = 0, \forall i$.

    This update can either be done \textit{asynchronously} or \textit{synchronously}, for the former,
    each $V_i$ is updated individually and the new $V_i$ is used for the updates
    of other nodes, and for the latter, all updates use the initial set of node spins
    before any updates took place. If updating synchronously, some kind of internal
    clock or way to save previous states of the network must exist, and so most 
    biological systems, such as the brain, will update asynchronously.
  \end{block}

  \begin{alertblock}{The Hebbian learning rule}

    The core of the Hopfield network lies in its use in \textit{'storing'} states
    of the network to be recovered later. This is done using the \textit{Hebbian learning
    rule}. \cite{hopfield:1982}

    If we have a set of $m$ states $V^s, s \in \{1, ..., m\}$, we can use the
    Hebbian learning rule:

    \begin{equation} \label{hebbian_learning_rule}
      T_{ij} = \sum_{s}{V^s_i}{V^s_j}
    \end{equation}

    Since this equation yields nonzero values for the case where $i = j$, we must
    then adjust for this by resetting $T_{ij}$ to $0$ whenever $i = j$. This will 
    update the weights of the network and effectively \textit{'imprint'}
    the states into the network.

    Now we have our basic tools for applying a Hopfield network to our problem, we
    can start looking at basic uses of the network.
  \end{alertblock}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Initialising and training the network}

    Suppose we have a set of $k$ $n$-by-$m$ uncorrupted images to train the model on,
    where each pixel can either be black or white.
    We can, without loss of generality, assign the colour black to the spin $+1$
    and white to $-1$. Now, each image can be represented as a set of $nm$ spins, 
    using each pixel as a node in the network, so we can use a Hopfield network to 
    store them.

    To set up our problem, we can store each image as a vector of its spins, so for
    the $s$th image:
    \begin{equation*}
      \boldsymbol{V^s} = \begin{bmatrix}
        V^s_1 \\
        V^s_2 \\
        \vdots \\
        V^s_{nm}
      \end{bmatrix}
    \end{equation*}

    Similarly, we also store the weights in an $nm$-by-$nm$ matrix:
    \begin{equation*}
      \boldsymbol{T} = \begin{bmatrix}
        T_{1,1}   & \dots   & T_{1,nm}\\
        \vdots      & \ddots  & \\
        T_{nm,1}  &         & T_{nm,nm}
      \end{bmatrix}
    \end{equation*}

    Then, we can apply the Hebbian learning rule to train the network on our images, using
    (\ref{hebbian_learning_rule}):
    \begin{equation} \label{hebbian_simple_eq}
      \boldsymbol{T} = \frac{1}{k} \sum_{s}(\boldsymbol{V^s}\otimes\boldsymbol{V^s} - diag({V^s_1}{V^s_1}, \dots, {V^s_{nm}}{V^s_{nm}}))
    \end{equation}
    where $\otimes$ indicates the vector outer product.

    This then gives us:
    \begin{equation*}
      \boldsymbol{T} = \frac{1}{k} \sum_{s} \begin{bmatrix}
        0                 & {V^s_1}{V^s_2}  & \dots   & {V^s_1}{V^s_{nm}}\\
        {V^s_2}{V^s_1}    & \ddots          &         & \vdots \\
        \vdots            &                 &         & \vdots \\
        {V^s_{nm}}{V^s_1} & \dots           & \dots   & 0
      \end{bmatrix}
    \end{equation*}
    as desired.

  \end{block}

  \begin{block}{Restoring images}

    Et rutrum ex euismod vel. Pellentesque ultricies, velit in fermentum
    vestibulum, lectus nisi pretium nibh, sit amet aliquam lectus augue vel
    velit. Suspendisse rhoncus massa porttitor augue feugiat molestie. Sed
    molestie ut orci nec malesuada. Sed ultricies feugiat est fringilla
    posuere.

    \begin{figure}
      \centering
      \begin{tikzpicture}
        \begin{axis}[
            scale only axis,
            no markers,
            domain=0:2*pi,
            samples=100,
            axis lines=center,
            axis line style={-},
            ticks=none]
          \addplot[red] {sin(deg(x))};
          \addplot[blue] {cos(deg(x))};
        \end{axis}
      \end{tikzpicture}
      \caption{Another figure caption.}
    \end{figure}

  \end{block}

  \begin{block}{t}

    Etiam sit amet tempus lorem, aliquet condimentum velit. Donec et nibh
    consequat, sagittis ex eget, dictum orci. Etiam quis semper ante. Ut eu
    mauris purus. Proin nec consectetur ligula. Mauris pretium molestie
    ullamcorper. Integer nisi neque, aliquet et odio non, sagittis porta justo.

  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{exampleblock}{A highlighted block containing some math}

    A different kind of highlighted block.

    $$
    \int_{-\infty}^{\infty} e^{-x^2}\,dx = \sqrt{\pi}
    $$

    Interdum et malesuada fames $\{1, 4, 9, \ldots\}$ ac ante ipsum primis in
    faucibus. Cras eleifend dolor eu nulla suscipit suscipit. Sed lobortis non
    felis id vulputate.

    \heading{A heading inside a block}

    Praesent consectetur mi $x^2 + y^2$ metus, nec vestibulum justo viverra
    nec. Proin eget nulla pretium, egestas magna aliquam, mollis neque. Vivamus
    dictum $\mathbf{u}^\intercal\mathbf{v}$ sagittis odio, vel porta erat
    congue sed. Maecenas ut dolor quis arcu auctor porttitor.

    \heading{Another heading inside a block}

    Sed augue erat, scelerisque a purus ultricies, placerat porttitor neque.
    Donec $P(y \mid x)$ fermentum consectetur $\nabla_x P(y \mid x)$ sapien
    sagittis egestas. Duis eget leo euismod nunc viverra imperdiet nec id
    justo.

  \end{exampleblock}

  \begin{block}{Nullam vel erat at velit convallis laoreet}

    Class aptent taciti sociosqu ad litora torquent per conubia nostra, per
    inceptos himenaeos. Phasellus libero enim, gravida sed erat sit amet,
    scelerisque congue diam. Fusce dapibus dui ut augue pulvinar iaculis.

    \begin{table}
      \centering
      \begin{tabular}{l r r c}
        \toprule
        \textbf{First column} & \textbf{Second column} & \textbf{Third column} & \textbf{Fourth} \\
        \midrule
        Foo & 13.37 & 384,394 & $\alpha$ \\
        Bar & 2.17 & 1,392 & $\beta$ \\
        Baz & 3.14 & 83,742 & $\delta$ \\
        Qux & 7.59 & 974 & $\gamma$ \\
        \bottomrule
      \end{tabular}
      \caption{A table caption.}
    \end{table}

    Donec quis posuere ligula. Nunc feugiat elit a mi malesuada consequat. Sed
    imperdiet augue ac nibh aliquet tristique. Aenean eu tortor vulputate,
    eleifend lorem in, dictum urna. Proin auctor ante in augue tincidunt
    tempor. Proin pellentesque vulputate odio, ac gravida nulla posuere
    efficitur. Aenean at velit vel dolor blandit molestie. Mauris laoreet
    commodo quam, non luctus nibh ullamcorper in. Class aptent taciti sociosqu
    ad litora torquent per conubia nostra, per inceptos himenaeos.

    Nulla varius finibus volutpat. Mauris molestie lorem tincidunt, iaculis
    libero at, gravida ante. Phasellus at felis eu neque suscipit suscipit.
    Integer ullamcorper, dui nec pretium ornare, urna dolor consequat libero,
    in feugiat elit lorem euismod lacus. Pellentesque sit amet dolor mollis,
    auctor urna non, tempus sem.

  \end{block}

  \begin{block}{References}
    \footnotesize{\bibliographystyle{plain}\bibliography{refs}}

  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}